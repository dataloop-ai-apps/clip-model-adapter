{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # CLIP Model Adapter Tutorial\n",
    "\n",
    "\n",
    "\n",
    " This notebook provides a step-by-step tutorial for using the CLIP model adapter to fine-tune and embed.\n",
    "\n",
    " There are two main steps in finetuning:\n",
    "\n",
    " 1. Preparing the dataset with descriptions\n",
    "\n",
    " 2. Running the model adapter for training and embedding\n",
    "\n",
    "\n",
    "\n",
    " Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import string\n",
    "import dtlpy as dl\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Set Up Dataloop Environment\n",
    "\n",
    "\n",
    "\n",
    " First, we need to set up our Dataloop environment and get our project. You'll need to replace project and dataset names with your own values.\n",
    "\n",
    "> **_NOTE:_**  This tutorial assumes you are working in a new project which does NOT have the CLIP model previously installed. If it's an existing project and you already have CLIP installed, you will need to get the appropriate app and base CLIP model entity for the rest of the code to work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dl.token_expired():\n",
    "    dl.login()\n",
    "dl.setenv(\"prod\") #TODO DELETE ME\n",
    "PROJECT_NAME = \"test clip FT8\" #\"<your project name here>\"\n",
    "project = dl.projects.create(project_name=PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Prepare Dataset with Descriptions\n",
    "\n",
    "\n",
    "This stage has two steps: first create the image dataset with uploaded descriptions, then convert into prompts and responses in prompt items. \n",
    "\n",
    "For this tutorial we will install the Mars Surface Images Datasets from the Dataloop Marketplace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command Progress: 100%|██████████| 100/100 [00:02<00:00, 45.37it/s]\n",
      "Mars Surface Datasets installed: Mars Surface Images\n"
     ]
    }
   ],
   "source": [
    "dpk = dl.dpks.get(dpk_name=\"mars-surface-images\")\n",
    "app = project.apps.install(dpk=dpk)\n",
    "print(f\"Mars Surface Datasets installed: {app.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets get the captioned dataset and split the data for training.  You may need to wait a few minutes after installing the app until the dataset has completed loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command Progress: 100%|██████████| 100/100 [00:06<00:00, 15.94it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = project.datasets.get(dataset_name=\"Mars Surface Images with Captions\")\n",
    "\n",
    "SUBSET_PERCENTAGES = {'train': 80, 'validation': 10, 'test': 10}\n",
    "dataset.split_ml_subsets(\n",
    "        items_query=dl.Filters(field='type', values='file'),\n",
    "        percentages=SUBSET_PERCENTAGES\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, if you'd like to upload your own dataset you can use the function code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_dataset(dataset_name, pairs_df, subset_percentages={'train': 60, 'validation': 20, 'test': 20}):\n",
    "    \"\"\"\n",
    "    Creates a new dataset from a CSV file containing image paths and descriptions\n",
    "\n",
    "    Args:\n",
    "        dataset_name (str): Name of the dataset to create\n",
    "        pairs_df (pd.DataFrame): DataFrame containing 'filepath' and 'img_description' columns\n",
    "        subset_percentages (dict): Dictionary containing the percentages for each subset\n",
    "        default is 60% train, 20% validation, 20% test\n",
    "        can be changed to any other percentages as long as the sum is 100\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        dataset = project.datasets.create(dataset_name=dataset_name)\n",
    "    except dl.exceptions.BadRequest:\n",
    "        # Generate 5 random alphanumeric characters\n",
    "        suffix = ''.join(random.choices(string.ascii_letters + string.digits, k=5))\n",
    "        dataset = project.datasets.create(dataset_name=f\"{dataset_name}_{suffix}\")\n",
    "\n",
    "    def upload_item(row):\n",
    "        file_path = row[\"filepath\"]\n",
    "        annots_path = file_path.replace(\"items\", \"json\")\n",
    "        \n",
    "        # Upload item with annotations\n",
    "        item = dataset.items.upload(\n",
    "            local_path=file_path,\n",
    "            local_annotations_path=annots_path,\n",
    "            item_metadata=dl.ExportMetadata.FROM_JSON,\n",
    "            overwrite=True,\n",
    "        )\n",
    "\n",
    "        # Set description and update\n",
    "        item.set_description(text=row[\"description\"])\n",
    "        item.update()\n",
    "\n",
    "    # Use ThreadPoolExecutor to upload items in parallel with progress bar\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        from tqdm import tqdm\n",
    "        list(tqdm(\n",
    "            executor.map(upload_item, [row for _, row in pairs_df.iterrows()]),\n",
    "            total=len(pairs_df),\n",
    "            desc=\"Uploading items\",\n",
    "            unit=\"item\",\n",
    "            bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]'\n",
    "        ))\n",
    "\n",
    "    # Since model training requires labels, we create a dummy label for the recipe\n",
    "    dataset.add_labels(label_list=['free-text'])\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next section is copied directly from the [CLIP model adapter repo](https://github.com/dataloop-ai-apps/clip-model-adapter/blob/main/utils/prepare_dataset.py), under `utils/prepare_dataset.py`. The `ClipPrepare` class contains functions to help us convert an existing dataset into a prompt item dataset, making it possible to train CLIP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipPrepare:\n",
    "    @staticmethod\n",
    "    def convert_dataset(dataset, keep_subsets=None):\n",
    "        dataset_to = ClipPrepare.convert_to_prompt_dataset(dataset_from=dataset, keep_subsets=keep_subsets)\n",
    "        return dataset_to\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_to_prompt_dataset(dataset_from: dl.Dataset, keep_subsets):\n",
    "        items = dataset_from.items.list()\n",
    "        try:\n",
    "            dataset_to = dataset_from.project.datasets.get(dataset_name=f\"{dataset_from.name} prompt items\")\n",
    "            if dataset_to.items_count > 0:\n",
    "                suffix = ''.join(random.choice(string.ascii_letters + string.digits) for _ in range(5))\n",
    "                dataset_to = dataset_from.project.datasets.create(\n",
    "                    dataset_name=f\"{dataset_from.name} prompt items-{suffix}\")\n",
    "        except Exception as e:\n",
    "            print(\"Prompt item dataset does not exist or already contains items. Creating new prompt item dataset.\")\n",
    "            suffix = ''.join(random.choice(string.ascii_letters + string.digits) for _ in range(5))\n",
    "            dataset_to = dataset_from.project.datasets.create(dataset_name=f\"{dataset_from.name} prompt items-{suffix}\")\n",
    "\n",
    "        # use thread multiprocessing to get items and convert them to prompt items\n",
    "        all_items = items.all()\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            _ = executor.map(lambda item: ClipPrepare._convert_item(item_id=item.id, dataset=dataset_to, existing_subsets=keep_subsets), all_items)\n",
    "\n",
    "        new_recipe = dataset_from.get_recipe_ids()[0]\n",
    "        dataset_to.switch_recipe(new_recipe)\n",
    "        return dataset_to\n",
    "\n",
    "    # add captions for the item either from description or from directory name\n",
    "    @staticmethod\n",
    "    def _convert_item(item_id, dataset: dl.Dataset, existing_subsets=True):\n",
    "        item = dl.items.get(item_id=item_id)\n",
    "        if item.description is not None:\n",
    "            caption = item.description\n",
    "        else:\n",
    "            print(f\"Item {item.id} has no description. Trying directory name.\")\n",
    "            item_dir = item.dir.split('/')[-1]\n",
    "            if item_dir != '':\n",
    "                print(f\"Using directory name: {item_dir}\")\n",
    "                caption = \"this is a photo of a \" + item_dir\n",
    "            else:\n",
    "                print(f\"Item {item.id} has no directory name. Using empty string.\")\n",
    "                caption = ''\n",
    "        new_name = Path(item.name).stem + '.json'\n",
    "\n",
    "        prompt_item = dl.PromptItem(name=new_name)\n",
    "        prompt_item.add(message={\"content\": [{\"mimetype\": dl.PromptType.IMAGE,  # role default is user\n",
    "                                              \"value\": item.stream}]})\n",
    "        new_metadata = item.metadata\n",
    "        if existing_subsets is True:\n",
    "            new_metadata[\"system\"] = new_metadata.get(\"system\", {})\n",
    "            new_metadata[\"system\"][\"subsets\"] = item.metadata.get(\"system\", {}).get(\n",
    "                \"subsets\", {}\n",
    "            )\n",
    "\n",
    "        new_item = dataset.items.upload(\n",
    "            prompt_item,\n",
    "            remote_name=new_name,\n",
    "            remote_path=item.dir,\n",
    "            overwrite=True,\n",
    "            item_metadata=new_metadata,\n",
    "        )\n",
    "        prompt_item._item = new_item\n",
    "        prompt_item.add(message={\"role\": \"assistant\",\n",
    "                                 \"content\": [{\"mimetype\": dl.PromptType.TEXT,\n",
    "                                              \"value\": caption}]})\n",
    "\n",
    "        return new_item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all the functions are ready, you can create your dataset, upload all the images and descriptions, and convert it into a prompt item dataset. We upload the image first because the Dataloop prompt item entities cannot store images on their own. Images must be uploaded as their own items before creating the corresponding prompt item with the image as the prompt and the description as the text annotation prompt response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt item dataset does not exist or already contains items. Creating new prompt item dataset.\n",
      "Iterate Entity: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "prompt_dataset = ClipPrepare.convert_dataset(dataset=dataset, keep_subsets=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should have two datasets: one with the original images and descriptions, and one with the prompt items.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Run Model Adapter\n",
    "\n",
    "\n",
    "\n",
    " Now that we have our dataset prepared, we can use the CLIP model adapter for training and embedding.\n",
    "\n",
    " First lets install the CLIP dpk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command Progress: 100%|██████████| 100/100 [00:02<00:00, 45.53it/s]\n",
      "CLIP App installed: OpenAI CLIP\n"
     ]
    }
   ],
   "source": [
    "dpk = dl.dpks.get(dpk_name='clip-model-pretrained')\n",
    "app = project.apps.install(dpk=dpk)\n",
    "print(f\"CLIP App installed: {app.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to clone the base CLIP model entity to prepare a new one for finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = project.models.get(model_name=\"openai-clip\")\n",
    "\n",
    "# Configure model metadata and subsets\n",
    "base_model.metadata[\"system\"] = {}\n",
    "base_model.metadata[\"system\"][\"subsets\"] = {}\n",
    "\n",
    "train_filters = dl.Filters(field=\"metadata.system.tags.train\", values=True)\n",
    "val_filters = dl.Filters(field=\"metadata.system.tags.validation\", values=True)\n",
    "\n",
    "base_model.metadata[\"system\"][\"subsets\"][\"train\"] = train_filters.prepare()\n",
    "base_model.metadata[\"system\"][\"subsets\"][\"validation\"] = val_filters.prepare()\n",
    "\n",
    "# Set model configuration (optional)\n",
    "base_model.configuration = {\n",
    "    \"model_name\": \"ViT-B/32\",\n",
    "    \"embeddings_size\": 512,\n",
    "    \"num_epochs\": 50,\n",
    "    \"batch_size\": 200,\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"early_stopping\": True,\n",
    "    \"early_stopping_epochs\": 5,\n",
    "}\n",
    "base_model.output_type = \"text\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can clone the pretrained CLIP model entity, set our dataset on the new model, and train. \n",
    "\n",
    "> **NOTE**: The training process might take some time depending on your dataset size and model configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model_name = base_model.name + \"-finetuned\"\n",
    "finetuned_model = base_model.clone(model_name=finetuned_model_name, dataset=prompt_dataset)\n",
    "execution = finetuned_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for training to complete...\n",
      "Training in progress... checking again in 5 minutes\n"
     ]
    }
   ],
   "source": [
    "# Wait for training to complete\n",
    "print(\"Waiting for training to complete...\")\n",
    "\n",
    "while execution.in_progress():\n",
    "    print(\"Training in progress... checking again in 5 minutes\")\n",
    "    time.sleep(300)  # Sleep for 5 minutes\n",
    "    execution = dl.executions.get(execution_id=execution.id)\n",
    "\n",
    "if execution.get_latest_status()['status'] == \"success\":\n",
    "    print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model has completed training, you can deploy your model and embed your images for better semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Service(created_at='2025-06-10T14:26:38.877Z', creator='yaya.t@dataloop.ai', version='1.0.0', package_id='68359a1d8101e8ec39d63dfd', package_revision='1.0.2', bot='bot.441a1408-133b-4338-abe7-585e3de058b0@bot.dataloop.ai', init_input={'model_entity': '68483ee8ca62bef2c6b2caf6'}, module_name='clip-module', name='predict-68483ee8ca62bef2c6b2ca-2r11', url='https://gate.dataloop.ai/api/v1/services/6848409e3c8bd8deabde2e04', id='6848409e3c8bd8deabde2e04', active=True, queue_length_limit=None, run_execution_as_process=False, execution_timeout=3600, drain_time=600, on_reset='failed', _type=None, project_id='5f1545c9-57b4-4821-82d6-adbb644d3d3b', org_id='778686bd-7941-4903-b2d5-ba638d8fabc8', is_global=False, max_attempts=3, metadata={'ml': {'modelId': '68483ee8ca62bef2c6b2caf6', 'modelOperation': 'deploy'}}, updated_by=None, app={'id': '68483eb7e544ef904680d4f3', 'dpkName': 'clip-model-pretrained', 'dpkVersion': '1.0.2', 'dpkId': '67a4bb4d4a998a3ee9dfcdd7'}, integrations=[])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuned_model.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may take a minute for the model to successfully deploy. Once the model is deployed and the service is up, you can embed your dataset. You'll need to get the updated model entity from the platform before embedding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command Progress: 100%|██████████| 100/100 [00:02<00:00, 44.99it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Command(id='684840c04e7979cf89bee10a', status='success', created_at='2025-06-10T14:27:12.754Z', updated_at='2025-06-10T14:27:12.979Z', type='EmbedDatasetsCommandSettings', progress=100, spec={'datasetIds': ['68483ead292a9f3bf0d844c3'], 'config': {'serviceId': '68484081f79dac7c437b9763'}, 'modelId': '68483ee8ca62bef2c6b2caf6', 'attachTrigger': False}, error=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuned_model = project.models.get(model_id=finetuned_model.id)\n",
    "finetuned_model.embed_datasets(dataset_ids=[dataset.id])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
