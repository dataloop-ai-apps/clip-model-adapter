{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # CLIP Model Adapter Tutorial\n",
    "\n",
    "\n",
    "\n",
    " This notebook provides a step-by-step tutorial for using the CLIP model adapter to fine-tune and embed.\n",
    "\n",
    " There are two main steps in finetuning:\n",
    "\n",
    " 1. Preparing the dataset with descriptions\n",
    "\n",
    " 2. Running the model adapter for training and embedding\n",
    "\n",
    "\n",
    "\n",
    " Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import string\n",
    "import dtlpy as dl\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Set Up Dataloop Environment\n",
    "\n",
    "\n",
    "\n",
    " First, we need to set up our Dataloop environment and get our project. You'll need to replace project and dataset names with your own values.\n",
    "\n",
    "> **_NOTE:_**  This tutorial assumes you are working in a new project which does NOT have the CLIP model previously installed. If you do, you will need to get the appropriate app installation and base CLIP model entity for the rest of the code to work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dl.token_expired():\n",
    "    dl.login()\n",
    "\n",
    "PROJECT_NAME = \"<your project name here>\"\n",
    "project = dl.projects.create(project_name=PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Prepare Dataset with Descriptions\n",
    "\n",
    "\n",
    "This stage has two steps: first create the image dataset with uploaded descriptions, then convert into prompts and responses in prompt items. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_dataset(dataset_name, pairs_df, subset_percentages={'train': 60, 'validation': 20, 'test': 20}):\n",
    "    \"\"\"\n",
    "    Creates a new dataset from a CSV file containing image paths and descriptions\n",
    "\n",
    "    Args:\n",
    "        dataset_name (str): Name of the dataset to create\n",
    "        pairs_df (pd.DataFrame): DataFrame containing 'filepath' and 'img_description' columns\n",
    "        subset_percentages (dict): Dictionary containing the percentages for each subset\n",
    "        default is 60% train, 20% validation, 20% test\n",
    "        can be changed to any other percentages as long as the sum is 100\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        dataset = project.datasets.create(dataset_name=dataset_name)\n",
    "    except dl.exceptions.BadRequest:\n",
    "        # Generate 5 random alphanumeric characters\n",
    "        suffix = ''.join(random.choices(string.ascii_letters + string.digits, k=5))\n",
    "        dataset = project.datasets.create(dataset_name=f\"{dataset_name}_{suffix}\")\n",
    "\n",
    "    def upload_item(row):\n",
    "        file_path = row[\"filepath\"]\n",
    "        annots_path = file_path.replace(\"items\", \"json\")\n",
    "        \n",
    "        # Upload item with annotations\n",
    "        item = dataset.items.upload(\n",
    "            local_path=file_path,\n",
    "            local_annotations_path=annots_path,\n",
    "            item_metadata=dl.ExportMetadata.FROM_JSON,\n",
    "            overwrite=True,\n",
    "        )\n",
    "\n",
    "        # Set description and update\n",
    "        item.set_description(text=row[\"description\"])\n",
    "        item.update()\n",
    "\n",
    "    # Use ThreadPoolExecutor to upload items in parallel with progress bar\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        from tqdm import tqdm\n",
    "        list(tqdm(\n",
    "            executor.map(upload_item, [row for _, row in pairs_df.iterrows()]),\n",
    "            total=len(pairs_df),\n",
    "            desc=\"Uploading items\",\n",
    "            unit=\"item\",\n",
    "            bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]'\n",
    "        ))\n",
    "\n",
    "    filters = dl.Filters(field='type', values='file')\n",
    "    dataset.split_ml_subsets(\n",
    "        items_query=filters,\n",
    "        percentages=subset_percentages\n",
    "    )\n",
    "    print(f\"Dataset split complete: {subset_percentages}\")\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"<your dataset name here>\"\n",
    "PAIRS_CSV_PATH = \"<path to your local csv file here>\"\n",
    "SUBSET_PERCENTAGES = {'train': 80, 'validation': 10, 'test': 10}\n",
    "dataset = create_new_dataset(dataset_name=DATASET_NAME, pairs_df=pd.read_csv(PAIRS_CSV_PATH), subset_percentages=SUBSET_PERCENTAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since training requires a label, we create a dummy label for the recipe\n",
    "dataset.add_labels(label_list=['free-text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next section is copied directly from the [CLIP model adapter repo](https://github.com/dataloop-ai-apps/clip-model-adapter/blob/main/utils/prepare_dataset.py), under `utils/prepare_dataset.py`. The `ClipPrepare` class contains functions to help us convert an existing dataset into a prompt item dataset, making it possible to train CLIP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipPrepare:\n",
    "    @staticmethod\n",
    "    def convert_dataset(dataset, keep_subsets=None):\n",
    "        dataset_to = ClipPrepare.convert_to_prompt_dataset(dataset_from=dataset, keep_subsets=keep_subsets)\n",
    "        return dataset_to\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_to_prompt_dataset(dataset_from: dl.Dataset, keep_subsets):\n",
    "        items = dataset_from.items.list()\n",
    "        try:\n",
    "            dataset_to = dataset_from.project.datasets.get(dataset_name=f\"{dataset_from.name} prompt items\")\n",
    "            if dataset_to.items_count > 0:\n",
    "                suffix = ''.join(random.choice(string.ascii_letters + string.digits) for _ in range(5))\n",
    "                dataset_to = dataset_from.project.datasets.create(\n",
    "                    dataset_name=f\"{dataset_from.name} prompt items-{suffix}\")\n",
    "        except Exception as e:\n",
    "            print(\"Prompt item dataset does not exist or already contains items. Creating new prompt item dataset.\")\n",
    "            suffix = ''.join(random.choice(string.ascii_letters + string.digits) for _ in range(5))\n",
    "            dataset_to = dataset_from.project.datasets.create(dataset_name=f\"{dataset_from.name} prompt items-{suffix}\")\n",
    "\n",
    "        # use thread multiprocessing to get items and convert them to prompt items\n",
    "        all_items = items.all()\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            _ = executor.map(lambda item: ClipPrepare._convert_item(item_id=item.id, dataset=dataset_to, existing_subsets=keep_subsets), all_items)\n",
    "        # for item in items.all():\n",
    "        #     item = dataset_from.items.get(item_id=item.id)\n",
    "        #     _ = _convert_item(item, dataset_to)\n",
    "\n",
    "        new_recipe = dataset_from.get_recipe_ids()[0]\n",
    "        dataset_to.switch_recipe(new_recipe)\n",
    "        return dataset_to\n",
    "\n",
    "    # add captions for the item either from description or from directory name\n",
    "    @staticmethod\n",
    "    def _convert_item(item_id, dataset: dl.Dataset, existing_subsets=True):\n",
    "        item = dl.items.get(item_id=item_id)\n",
    "        if item.description is not None:\n",
    "            caption = item.description\n",
    "        else:\n",
    "            print(f\"Item {item.id} has no description. Trying directory name.\")\n",
    "            item_dir = item.dir.split('/')[-1]\n",
    "            if item_dir != '':\n",
    "                print(f\"Using directory name: {item_dir}\")\n",
    "                caption = \"this is a photo of a \" + item_dir\n",
    "            else:\n",
    "                print(f\"Item {item.id} has no directory name. Using empty string.\")\n",
    "                caption = ''\n",
    "        new_name = Path(item.name).stem + '.json'\n",
    "\n",
    "        prompt_item = dl.PromptItem(name=new_name)\n",
    "        prompt_item.add(message={\"content\": [{\"mimetype\": dl.PromptType.IMAGE,  # role default is user\n",
    "                                              \"value\": item.stream}]})\n",
    "        new_metadata = item.metadata\n",
    "        if existing_subsets is True:\n",
    "            new_metadata[\"system\"] = new_metadata.get(\"system\", {})\n",
    "            new_metadata[\"system\"][\"subsets\"] = item.metadata.get(\"system\", {}).get(\n",
    "                \"subsets\", {}\n",
    "            )\n",
    "\n",
    "        new_item = dataset.items.upload(\n",
    "            prompt_item,\n",
    "            remote_name=new_name,\n",
    "            remote_path=item.dir,\n",
    "            overwrite=True,\n",
    "            item_metadata=new_metadata,\n",
    "        )\n",
    "        prompt_item._item = new_item\n",
    "        prompt_item.add(message={\"role\": \"assistant\",\n",
    "                                 \"content\": [{\"mimetype\": dl.PromptType.TEXT,\n",
    "                                              \"value\": caption}]})\n",
    "\n",
    "        return new_item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all the functions are ready, you can create your dataset, upload all the images and descriptions, and convert it into a prompt item dataset. We upload the image first because the Dataloop prompt item entities cannot store images on their own. Images must be uploaded as their own items before creating the corresponding prompt item with the image as the prompt and the description as the text annotation prompt response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_dataset = ClipPrepare.convert_dataset(dataset=dataset, keep_subsets=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should have two datasets: one with the original images and descriptions, and one with the prompt items.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Run Model Adapter\n",
    "\n",
    "\n",
    "\n",
    " Now that we have our dataset prepared, we can use the CLIP model adapter for training and embedding.\n",
    "\n",
    " First lets install the CLIP dpk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = dl.Filters(resource=dl.FiltersResource.DPK)\n",
    "filters.add(field=\"name\", values=\"clip-model-pretrained\")\n",
    "\n",
    "dpks = list(dl.dpks.list(filters=filters).all())\n",
    "app = project.apps.install(dpk=dpks[0])\n",
    "\n",
    "print(f\"CLIP App installed: {app.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to clone the base CLIP model entity to prepare a new one for finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = project.models.get(model_name=\"openai-clip\")\n",
    "\n",
    "# Configure model metadata and subsets\n",
    "base_model.metadata[\"system\"] = {}\n",
    "base_model.metadata[\"system\"][\"subsets\"] = {}\n",
    "\n",
    "train_filters = dl.Filters(field=\"metadata.system.tags.train\", values=True)\n",
    "val_filters = dl.Filters(field=\"metadata.system.tags.validation\", values=True)\n",
    "\n",
    "base_model.metadata[\"system\"][\"subsets\"][\"train\"] = train_filters.prepare()\n",
    "base_model.metadata[\"system\"][\"subsets\"][\"validation\"] = val_filters.prepare()\n",
    "\n",
    "# Set model configuration (optional)\n",
    "base_model.configuration = {\n",
    "    \"model_name\": \"ViT-B/32\",\n",
    "    \"embeddings_size\": 512,\n",
    "    \"num_epochs\": 50,\n",
    "    \"batch_size\": 200,\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"early_stopping\": True,\n",
    "    \"early_stopping_epochs\": 5,\n",
    "}\n",
    "base_model.output_type = \"text\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone and train the model\n",
    "finetuned_model_name = base_model.name + \"-finetuned\"\n",
    "finetuned_model = base_model.clone(model_name=finetuned_model_name, dataset=prompt_dataset)\n",
    "execution = finetuned_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for training to complete\n",
    "print(\"Waiting for training to complete...\")\n",
    "\n",
    "while execution.in_progress():\n",
    "    print(\"Training in progress... checking again in 5 minutes\")\n",
    "    time.sleep(300)  # Sleep for 5 minutes\n",
    "    execution = dl.executions.get(execution_id=execution.id)\n",
    "\n",
    "if execution.get_latest_status()['status'] == \"success\":\n",
    "    print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model has completed training, you can deploy your model and embed your images for better semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model.deploy()\n",
    "finetuned_model.embed_datasets(dataset_ids=[dataset.id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Important Notes\n",
    "\n",
    "\n",
    "\n",
    " 1. Make sure you have the required dependencies installed:\n",
    "\n",
    "    - dtlpy\n",
    "\n",
    "    - pandas\n",
    "\n",
    "    - model_adapter (from your project)\n",
    "\n",
    "\n",
    "\n",
    " 2. The environment variables (ENV, PROJECT_NAME, etc.) should be set according to your specific setup.\n",
    "\n",
    "\n",
    "\n",
    " 3. The training process might take some time depending on your dataset size and model configuration.\n",
    "\n",
    "\n",
    "\n",
    " 4. Make sure you have proper access to the Dataloop platform and the required permissions in your project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
